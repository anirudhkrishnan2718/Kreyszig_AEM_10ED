\section{Symmetric, Skew-Symmetric, and Orthogonal Matrices}

\begin{enumerate}
    \item The matrix is orthogonal
          \begin{align}
              \vec{A}       & = \bmattt{0.8}{0.6}{-0.6}{0.8}             &
              \vec{A}^{-1}  & = \bmattt{0.8}{-0.6}{0.6}{0.8} = \vec{A}^T   \\
              0             & = (\lambda - 0.8)^2 + 0.6^2                &
              \{\lambda_i\} & = \{0.8 \pm 0.6i\}                           \\
              |\lambda_i|   & = 1 \quad \forall \quad i
          \end{align}
          The eigenvalues are real or complex conjugate pairs, with absolute value 1.

    \item The matrix is not special
          \begin{align}
              \vec{A}       & = \bmattt{a}{b}{-b}{a}                   &
              \vec{A}^{-1}  & = \frac{1}{a^2 + b^2}
              \bmattt{a}{-b}{b}{a}                                       \\
              0             & = (\lambda - a)^2 + b^2                  &
              \{\lambda_i\} & = \{a \pm bi\}                             \\
              |\lambda_i|   & = \sqrt{a^2 + b^2} \quad \forall \quad i
          \end{align}
          Theorem 5 holds if $ a^2 + b^2 = 1 $, and the matrix becomes orthogonal.

    \item The matrix is not special
          \begin{align}
              \vec{A}       & = \bmattt{2}{8}{-8}{2}            &
              \vec{A}^{-1}  & = \frac{1}{\sqrt{68}}
              \bmattt{2}{-8}{8}{2}                                \\
              0             & = (\lambda - 2)^2 + 8^2           &
              \{\lambda_i\} & = \{2 \pm 8i\}                      \\
              |\lambda_i|   & = \sqrt{68} \quad \forall \quad i
          \end{align}

    \item The matrix is orthogonal
          \begin{align}
              \vec{A}                    & = \bmattt{\cos \theta}{-\sin \theta}
              {\sin \theta}{\cos \theta} &
              \vec{A}^{-1}               & = \bmattt{\cos \theta}{\sin \theta}
              {-\sin \theta}{\cos \theta}                                       \\
              0                          & = (\lambda - \cos \theta)^2
              + \sin^2 \theta            &
              \{\lambda_i\}              & = \{\cos \theta \pm \sin \theta\}    \\
              \abs{\lambda_i}            & = 1 \quad \forall \quad i
          \end{align}
          The eigenvalues are real or complex conjugate pairs, with absolute value 1.

    \item The matrix is symmetric
          \begin{align}
              \vec{A}                                    & =
              \begin{bNiceMatrix}[r, margin]
                  6 & 0  & 0  \\
                  0 & 2  & -2 \\
                  0 & -2 & 5
              \end{bNiceMatrix}             &
              \vec{A}^T                                  & =
              \begin{bNiceMatrix}[r, margin]
                  6 & 0  & 0  \\
                  0 & 2  & -2 \\
                  0 & -2 & 5
              \end{bNiceMatrix}                                          \\
              \vec{A}^{-1}                               & =
              \frac{1}{36}\begin{bNiceMatrix}[r, margin]
                              6 & 0  & 0  \\
                              0 & 30 & 12 \\
                              0 & 12 & 12
                          \end{bNiceMatrix} &
              \vec{A}^{-1}                               & =
              \begin{bNiceMatrix}[r, margin]
                  \frac{1}{6} & 0           & 0           \\
                  0           & \frac{5}{6} & \frac{1}{3} \\
                  0           & \frac{1}{3} & \frac{1}{3}
              \end{bNiceMatrix}                                \\
              0                                          & = \lambda^3 - 13\lambda^2
              + 48\lambda - 36                           &
              \{\lambda_i\}                              & = \{1,6,6\}
          \end{align}
          The eigenvalues are real.

    \item The matrix is symmetric
          \begin{align}
              \vec{A}       & = \begin{bNiceMatrix}[r, margin]
                                    a & k & k \\
                                    k & a & k \\
                                    k & k & a
                                \end{bNiceMatrix}                 \\
              \vec{A}^T     & = \begin{bNiceMatrix}[r, margin]
                                    a & k & k \\
                                    k & a & k \\
                                    k & k & a
                                \end{bNiceMatrix}                 \\
              \vec{A}^{-1}  & =
              \frac{1}{a^2 + ak - 2k^2} \begin{bNiceMatrix}[r, margin]
                                            (a + k) & -k      & -k      \\
                                            -k      & (a + k) & -k      \\
                                            -k      & -k      & (a + k)
                                        \end{bNiceMatrix}         \\
              0             & = \lambda^3 - 3a\lambda^2 + 3(a^2 - k^2)\lambda
              - (a^3 + 3ak^2 - 2k^3)                                          \\
              \{\lambda_i\} & = \{a + 2k, a-k, a-k\}
          \end{align}
          The eigenvalues are real.

    \item The matrix is skew-symmetric
          \begin{align}
              \vec{A}       & = \begin{bNiceMatrix}[r, margin]
                                    0  & 9   & -12 \\
                                    -9 & 0   & 20  \\
                                    12 & -20 & 0
                                \end{bNiceMatrix} &
              \vec{A}^T     & = \begin{bNiceMatrix}[r, margin]
                                    0   & -9 & 12  \\
                                    9   & 0  & -20 \\
                                    -12 & 20 & 0
                                \end{bNiceMatrix} = -\vec{A}    \\
              \det(\vec{A}) & = 0                              &
              \vec{A}^{-1}  & \ \text{does not exist}            \\
              0             & = \lambda^3 + 625\lambda         &
              \{\lambda_i\} & = \{0, 0 \pm 25i\}
          \end{align}
          The eigenvalues are either zero or purely imaginary.

    \item The matrix is orthogonal
          \begin{align}
              \vec{A}       & = \begin{bNiceMatrix}[r, margin]
                                    1 & 0           & 0            \\
                                    0 & \cos \theta & -\sin \theta \\
                                    0 & \sin \theta & \cos \theta
                                \end{bNiceMatrix}                    &
              \vec{A}^T     & = \begin{bNiceMatrix}[r, margin]
                                    1 & 0            & 0           \\
                                    0 & \cos \theta  & \sin \theta \\
                                    0 & -\sin \theta & \cos \theta
                                \end{bNiceMatrix}                       \\
              \det(\vec{A}) & = 1                                                  &
              \vec{A}^{-1}  & = \begin{bNiceMatrix}[r, margin]
                                    1 & 0            & 0           \\
                                    0 & \cos \theta  & \sin \theta \\
                                    0 & -\sin \theta & \cos \theta
                                \end{bNiceMatrix} = \vec{A}^T                       \\
              0             & = (1 - \lambda)(\lambda^2 - 2\lambda\cos \theta + 1) &
              \{\lambda_i\} & = \{1, \cos \theta \pm i \sin \theta\}
          \end{align}
          The eigenvalues are either real or complex conjuagate pairs with
          $ \abs{ \lambda_i} = 1 $.

    \item The matrix is orthogonal
          \begin{align}
              \vec{A}       & = \begin{bNiceMatrix}[r, margin]
                                    0  & 0 & 1 \\
                                    0  & 1 & 0 \\
                                    -1 & 0 & 0
                                \end{bNiceMatrix}      &
              \vec{A}^T     & = \begin{bNiceMatrix}[r, margin]
                                    0 & 0 & -1 \\
                                    0 & 1 & 0  \\
                                    1 & 0 & 0
                                \end{bNiceMatrix}         \\
              \det(\vec{A}) & = 1                                   &
              \vec{A}^{-1}  & = \begin{bNiceMatrix}[r, margin]
                                    0 & 0 & -1 \\
                                    0 & 1 & 0  \\
                                    1 & 0 & 0
                                \end{bNiceMatrix} = \vec{A}^T         \\
              0             & = \lambda^3 - \lambda^2 + \lambda - 1 &
              \{\lambda_i\} & = \{1, 0 \pm i\}
          \end{align}
          The eigenvalues are either real or complex conjuagate pairs with
          $ \abs{\lambda_i} = 1 $.

    \item The matrix is orthogonal
          \begin{align}
              \vec{A}                        & = \frac{1}{9}
              \begin{bNiceMatrix}[r, margin]
                  4  & 8 & 1  \\
                  -7 & 4 & -4 \\
                  -4 & 1 & 8
              \end{bNiceMatrix} &
              \vec{A}^T                      & = \frac{1}{9}
              \begin{bNiceMatrix}[r, margin]
                  4 & -7 & -4 \\
                  8 & 4  & 1  \\
                  1 & -4 & 8
              \end{bNiceMatrix}                                            \\
              \det(\vec{A})                  & = 1                                   &
              \vec{A}^{-1}                   & = \frac{1}{9}
              \begin{bNiceMatrix}[r, margin]
                  4 & -7 & -4 \\
                  8 & 4  & 1  \\
                  1 & -4 & 8
              \end{bNiceMatrix} = \vec{A}^T                                            \\
              0                              & = \lambda^3 - \frac{16}{9}\ \lambda^2
              + \frac{16}{9}\ \lambda - 1    &
              \{\lambda_i\}                  & = \left\{ 1,
              \frac{7}{18} \pm \frac{\sqrt{275}}{18}i \right\}
          \end{align}
          The eigenvalues are either real or complex conjuagate pairs with
          $ \abs{\lambda_i} = 1 $.

    \item Refer notes. Exmaples TBC. (Take from exercises)

    \item Properties of orthogonal matrices.
          \begin{enumerate}
              \item Let $ \vec{A}, \vec{B} $ be orthogonal,
                    \begin{align}
                        \vec{C}      & = \vec{AB}                 &
                        \vec{C}^T    & = \vec{B}^T \vec{A}^T        \\
                        \vec{C}^{-1} & = \vec{B}^{-1}\vec{A}^{-1} &
                        \vec{C}^{-1} & = \vec{C}^T
                    \end{align}
                    Their product is also orthogonal.

              \item Checking if the matrix is orthogonal,
                    \begin{align}
                        \vec{A}                    & = \bmattt{\cos \theta}
                        {-\sin \theta}
                        {\sin \theta}{\cos \theta} &
                        \vec{A}^{-1}               & = \bmattt{\cos \theta}
                        {\sin \theta}
                        {-\sin \theta}{\cos \theta}                                    \\
                        0                          & = (\lambda - \cos \theta)^2
                        + \sin^2 \theta            &
                        \{\lambda_i\}              & = \{\cos \theta \pm \sin \theta\} \\
                        \abs{\lambda_i}            & = 1 \quad \forall \quad i
                    \end{align}
                    Checking the orthonormality of rows,
                    \begin{align}
                        \vec{a}_1 \dotp \vec{a}_2 & = 0 &
                        \vec{a}_1 \dotp \vec{a}_1 & = 1   \\
                        \vec{a}_2 \dotp \vec{a}_2 & = 1
                    \end{align}
                    The rows and columns do form an orthonormal system. The inverse
                    transformation is rotation by angle $ -\theta $

              \item The eigenvalues of $ \vec{A}^m $ are $ \{\lambda_i^m\} $ as shown in
                    Problem 18, section 8.2
                    \begin{align}
                        \vec{A}       & = \bmattt{a}{b}{-b}{a}                   &
                        \vec{A}^{-1}  & = \frac{1}{a^2 + b^2}
                        \bmattt{a}{-b}{b}{a}                                       \\
                        0             & = (\lambda - a)^2 + b^2                  &
                        \{\lambda_i\} & = \{a \pm bi\}                             \\
                        |\lambda_i|   & = \sqrt{a^2 + b^2} \quad \forall \quad i
                    \end{align}
                    Assuming the matrix is orthogonal, defining
                    \begin{align}
                        \theta      & = \arctan\left( \frac{b}{a} \right) &
                        R           & = \sqrt{a^2 + b^2}                    \\
                        \lambda_1   & = R\exp(i\ \theta)                  &
                        \lambda_1^m & = R^m\exp(i\ m\theta)                 \\
                        \lambda_2   & = R\exp(-i\ \theta)                 &
                        \lambda_2^m & = R^m\exp(-i\ m\theta)
                    \end{align}
                    The matrix $ \vec{A} $ corresponds to rotation by
                    $ \SI{36.87}{\degree} $ in the clockwise direction. \par
                    There is no asymptotic eigenvalue since $ R = 1 $.

              \item Since $ R < 1 $, the eigenvalues decay to zero with increasing
                    $ m $.
                    \begin{align}
                        \lambda_1^m & = R^m \cos(m\theta) + i\ R^m\sin(m\theta) \\
                        \lambda_2^m & = R^m \cos(m\theta) - i\ R^m\sin(m\theta)
                    \end{align}
                    \begin{figure}[H]
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                    title = {$ \lambda_1 $},
                                    grid = both,
                                    xlabel = Real,
                                    ylabel = Imaginary,
                                    axis equal,
                                    width = 8cm,
                                    legend pos = north west,
                                    domain = 0:100,
                                    Ani]
                                \addplot[GraphSmooth, color = y_h]
                                ({0.9^(x) * cos(x * atan(3/4))},
                                {0.9^(x) * sin(x * atan(3/4))});
                            \end{axis}
                        \end{tikzpicture}
                        \begin{tikzpicture}
                            \begin{axis}[
                                    title = {$\lambda_2$},
                                    grid = both,
                                    xlabel = Real,
                                    ylabel = Imaginary,
                                    axis equal,
                                    width = 8cm,
                                    legend pos = north west,
                                    domain = 0:100,
                                    Ani]
                                \addplot[GraphSmooth, color = y_p]
                                ({0.9^(x) * cos(x * atan(3/4))},
                                {0.9^(x) * (-1) * sin(x * atan(3/4))});
                            \end{axis}
                        \end{tikzpicture}
                    \end{figure}

                    The eigenvalues approach the limit $ 0 \pm 0i $ along a spiral.

              \item Counter-clockwise rotation through $ \SI{30}{\degree} $ in
                    the 2-d plane,
                    \begin{align}
                        \vec{A} & = \bmattt{\frac{\sqrt{3}}{2}}{-\frac{1}{2}}
                        {\frac{1}{2}}{\frac{\sqrt{3}}{2}}
                    \end{align}
          \end{enumerate}

    \item Verifying,
          \begin{align}
              \vec{A}       & = \begin{bNiceMatrix}[r, margin]
                                    -3 & 1  & 5  \\
                                    1  & 0  & -2 \\
                                    5  & -2 & 4
                                \end{bNiceMatrix}            &
              \vec{A}^{T}   & = \begin{bNiceMatrix}[r, margin]
                                    -3 & 1  & 5  \\
                                    1  & 0  & -2 \\
                                    5  & -2 & 4
                                \end{bNiceMatrix} = \vec{A}               \\
              \vec{B}       & = \begin{bNiceMatrix}[r, margin]
                                    0  & 9   & -12 \\
                                    -9 & 0   & 20  \\
                                    12 & -20 & 0
                                \end{bNiceMatrix}            &
              \vec{B}^{T}   & = \begin{bNiceMatrix}[r, margin]
                                    0   & -9 & 12  \\
                                    9   & 0  & -20 \\
                                    -12 & 20 & 0
                                \end{bNiceMatrix} = -\vec{B}               \\
              \vec{C}       & = \frac{1}{3}\begin{bNiceMatrix}[r, margin]
                                               2  & 1 & 2  \\
                                               -2 & 2 & 1  \\
                                               1  & 2 & -2
                                           \end{bNiceMatrix} &
              \vec{C}^{T}   & = \frac{1}{3}\begin{bNiceMatrix}[r, margin]
                                               2  & -2 & 1  \\
                                               -2 & 2  & 2  \\
                                               1  & 1  & -2
                                           \end{bNiceMatrix}    \\
              \det(\vec{C}) & = -1                                        &
              \vec{C}^{-1}  & =\frac{1}{3}\begin{bNiceMatrix}[r, margin]
                                              2  & -2 & 1  \\
                                              -2 & 2  & 2  \\
                                              1  & 1  & -2
                                          \end{bNiceMatrix} = \vec{A}^T
          \end{align}

    \item Refer Problem 7 for skew-symmetric matrix. \par
          Checking the eigenvalues of the matrix,
          \begin{align}
              \vec{A}             & = \bmattt{3}{4}{1}{3} &
              (\lambda - 3)^2 - 4 & = 0                     \\
              \{\lambda_i\}       & = \{1, 5\}
          \end{align}
          This does not violate the theorem, because it only has the forward
          implication.
          \par
          Checking the given matrix,
          \begin{align}
              \vec{M}                                      & =
              \begin{bNiceMatrix}[r, margin]
                  \frac{2}{3}  & \frac{1}{3} & \frac{2}{3}  \\
                  -\frac{2}{3} & \frac{2}{3} & \frac{1}{3}  \\
                  \frac{1}{3}  & \frac{2}{3} & -\frac{2}{3} \\
              \end{bNiceMatrix} &
              \det(\vec{M})                                & =
              \frac{-12 + 2(-6) + (-3)}{27} = -1
          \end{align}
          Refer to Problem 4.

    \item Checking,
          \begin{align}
              \vec{Ax}_j & = \lambda_j \vec{x}_j &
              \vec{Bx}_k & = \mu_k \vec{x}_k
          \end{align}
          Only eigenvectors common to the two sets satisfy the required relation. So
          the relation is not true in general.

    \item Different eigenvalues of a symmetric matrix,
          \begin{align}
              \vec{Ax}_1                       & = \lambda_1 \vec{x}_1              &
              \vec{Ax}_2                       & = \lambda_2 \vec{x}_2                \\
              \vec{x}_1^T \vec{A}^T            & = \lambda_1 \vec{x}_1^T            &
              \vec{x}_1^T \vec{A}^T \vec{x}_2  & = \lambda_1\ \vec{x}_1^T \vec{x}_2   \\
              \vec{A}^T                        & = \vec{A}                          &
              \lambda_2\ \vec{x}_1^T \vec{x}_2 & = \lambda_1\ \vec{x}_1^T \vec{x}_2
          \end{align}
          Since the eigenvectors correspond to distinct eigenvalues, $ \lambda_2
              \neq \lambda_1 $.
          \begin{align}
              \lambda_1 \neq \lambda_2 & \implies \vec{x}_1^T \vec{x}_2 = 0
          \end{align}

    \item Let $ \vec{B}^T = -\vec{B} $,
          \begin{align}
              \vec{B}^{-1}\ \vec{B}                          & = \vec{I}  &
              \vec{B}^T\ \Big(\vec{B}^{-1}\Big)^T            & = \vec{I}    \\
              \vec{B}\ \Big(\vec{B}^{-1}\Big)^T              & = -\vec{I} &
              \vec{B}^{-1} \vec{B}\ \Big(\vec{B}^{-1}\Big)^T & =
              -\vec{B}^{-1} \vec{I}                                         \\
              \Big(\vec{B}^{-1}\Big)^T                       & =
              -\vec{B}^{-1}
          \end{align}
          Thus, the inverse of a skew-symmetric matrix is also skew-symmetric.

    \item Let $ \vec{A} $ be a skew-symmetric matrix of odd order,
          \begin{align}
              \vec{A}              & = -\vec{A}^T               &
              \det(\vec{A}^T)      & = \det(\vec{A})              \\
              \det(-\vec{A})       & = \det(\vec{A})            &
              (-1)^n \det(\vec{A}) & = \det(\vec{A})              \\
              \text{n odd}\        & \implies \det(\vec{A}) = 0
          \end{align}
          So, all such matrices are singular. The claim is \textcolor{y_p}{false}.

    \item Let the matrix be orthogonal
          \begin{align}
              \vec{A}^T     & = \vec{A}^{-1} &
              \det(\vec{A}) & \neq 0
          \end{align}
          From the result in Problem 18, skew-symmetric matrices of odd order cannot
          have an inverse, and thus cannot be orthogonal. So, the claim is
          \textcolor{y_p}{false}.

    \item The matrix is of odd order, symmetric, and orthogonal.
          \begin{align}
              \vec{A}^T        & = \vec{A}^{-1} &
              \vec{A}^T        & = \vec{A}        \\
              \implies \vec{A} & = \vec{A}^{-1}
          \end{align}
          All such matrices are diagonal. This makes the claim \textcolor{y_p}{false}.
\end{enumerate}